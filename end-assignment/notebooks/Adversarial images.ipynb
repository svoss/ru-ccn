{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_DEVICE = -1#Please set the GPU device you want to use to execute this notebook -1 means CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial images\n",
    "In this notebook we will have a look at the vulnerability for adversarial examples of the networks learned in 'training GAN' and 'training supervised network' notebooks. First we will use the supervised model to generate adversarial examples and see how vulnerable this network is to these examples. Next we will use the same examples to test the vulnerability of the descriminator learned by the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import chainer, os, collections, six, math, random, time, copy,sys\n",
    "from chainer import cuda, Variable, optimizers, serializers, function, optimizer, initializers\n",
    "from chainer.utils import type_check\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and constants\n",
    "Here i will define some helper functions that we might want for both the supervised network and the gan network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the epsilons value we will calculate adversarial images of i also generate them with epsilon 0.\n",
    "# This gives me the original images and make my life a little easier when obtaining the results\n",
    "GRADIENT_STEPS = [0.0,.1,.25,1.] \n",
    "\n",
    "# Column names for the to be generated csvs\n",
    "COLS = ['index','org_index','loss','eps','correct','found_label','confidence'] + ['prob_' + str(x) for x in range(10)]\n",
    "\n",
    "def _build_labels(p, label,eps):\n",
    "    \"\"\" Helper function to build label for under the image, shows the certainty \n",
    "    with which the image is classified as a certain integer\n",
    "    \"\"\"\n",
    "    return (\"%d(%.2f%%)\" % (label, p*100.0), \"$\\epsilon$:%.2f\" % eps)\n",
    "\n",
    "def _plot_image(i,ax, text_a,text_b):\n",
    "    \"\"\" Plots a gray scale image with some text under it\n",
    "    text_a is first line\n",
    "    text_b is second line\n",
    "    \"\"\"\n",
    "    i = i.reshape(28,28)\n",
    "    ax.imshow(i,cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.annotate(text_a, xy=(0, 0), xytext=(2, 32))\n",
    "    ax.annotate(text_b, xy=(0, 0), xytext=(7, 36))\n",
    "\n",
    "def plot_images(imgs,confidence,labels,eps=GRADIENT_STEPS):\n",
    "    \"\"\" Plots multiple images with the probablity of the correct class and the probablity of the target adv class\n",
    "    \"\"\"\n",
    "    ni = len(imgs)\n",
    "    fig, axarr = plt.subplots(ncols=ni)\n",
    "    for i in range(ni):\n",
    "        correct_label = labels[i]\n",
    "        p = confidence[i]\n",
    "        text_a,text_b = _build_labels(p, correct_label,eps[i])\n",
    "        _plot_image(imgs[i,:],axarr[i],text_a,text_b )\n",
    "    \n",
    "    \n",
    "def statistics_from_df(df):\n",
    "    \"\"\" Gets statistics(average loss, average confidence and accuracy) for every possible value of epsilon\n",
    "    return als list of tuples (eps, average loss, average confidence, average accuracy)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for eps in GRADIENT_STEPS:\n",
    "        # get all fields for this\n",
    "        s = df[df.eps == eps]\n",
    "        p = s[s.correct == s.found_label]\n",
    "        \n",
    "        acc = float(p.shape[0])/s.shape[0]\n",
    "        data.append((eps, acc, np.average(s.confidence.tolist()),np.average(s.loss.tolist())))\n",
    "    return data\n",
    "\n",
    "def statistics_to_latex_table(data):\n",
    "    \"\"\" Can be used to print data as table\n",
    "    \"\"\"\n",
    "    print \"\\\\begin{tabular}{|l|l|l|l|}\\hline\"\n",
    "    print \" & \".join([\"\\\\textbf{\"+x+\"}\" for x in ['type','accuracy','avg. confidence', 'avg. loss']]) + \"\\\\\\\\\\hline\"\n",
    "    for d in data:\n",
    "        X = list(d)\n",
    "        print  \" & \".join([\"$\\epsilon = %.2f$\" % d[0]] +[\"%.3f\" % x for x in X[1:]]) + \"\\\\\\\\\\hline\"\n",
    "    print \"\\end{tabular}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised network\n",
    "Here i will load the normal supervised neural network and see how it works out\n",
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network definition\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            l1=L.Linear(None, 1000),  # n_in -> n_units\n",
    "            l2=L.Linear(None, 500),  # n_units -> n_units\n",
    "            l3=L.Linear(None, 250),  # n_units -> n_units\n",
    "            l4=L.Linear(None, n_out),  # n_units -> n_out\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.elu(self.l1(x))\n",
    "        h2 = F.elu(self.l2(h1))\n",
    "        h3 = F.elu(self.l3(h2))\n",
    "        return self.l4(h3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = L.Classifier(MLP( 10))\n",
    "resume = 'result/model.npz'\n",
    "if GPU_DEVICE >= 0:\n",
    "    chainer.cuda.get_device(GPU_DEVICE).use()  # Make a specified GPU current\n",
    "    model.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "chainer.serializers.load_npz(resume, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions that help to translate between chainer variable and numpy variables\n",
    "def _to_variable(x):\n",
    "    if isinstance(x, Variable) == False:\n",
    "        x = Variable(x)\n",
    "        if GPU_DEVICE > -1:\n",
    "            x.to_gpu()\n",
    "    return x\n",
    "\n",
    "def _to_numpy(x):\n",
    "    if isinstance(x, Variable) == True:\n",
    "        x = x.data\n",
    "    if isinstance(x, cuda.ndarray) == True:\n",
    "        x = cuda.to_cpu(x)\n",
    "    return x   \n",
    "\n",
    "def pass_forward_sup(ims, target_labels, calc_gradients = False):\n",
    "    \"\"\" This passes images forward trough the network. Using the target_labels \n",
    "    it will also calculate the loss. If calc_gradients is set to True it will \n",
    "    calculate the gradiens with respect to the input\n",
    "    \n",
    "    Returns a tuple containing loss, probabilities and optionally gradients\n",
    "    \"\"\"\n",
    "    ims = _to_variable(ims)\n",
    "    \n",
    "    target = _to_variable(target_labels)\n",
    "    # predict loss\n",
    "    loss = model(ims, target)\n",
    "    if calc_gradients:\n",
    "        # back propagate error\n",
    "        loss.backward()\n",
    "        \n",
    "        return _to_numpy(loss), _to_numpy(F.softmax(model.y)), ims.grad\n",
    "    else:\n",
    "        # model.predictor contains our original model, without the classifier class\n",
    "        return _to_numpy(loss), _to_numpy(F.softmax(model.predictor(ims)))\n",
    "        \n",
    "def create_adv_images(ims, sgrad, eps):\n",
    "    \"\"\" Given original images, signed gradient matrix and a epsilon will \n",
    "    calculate the new adversarial images.\n",
    "    Will also make sure the matrix is constraint to a minimum of 0 and a max of 1\n",
    "    \"\"\"\n",
    "    return np.minimum(\n",
    "        np.ones(shape=ims.shape, dtype=np.float32), \n",
    "        np.maximum(\n",
    "            np.zeros(shape=ims.shape,dtype=np.float32), \n",
    "            ims + np.multiply(eps,sgrad)\n",
    "        )\n",
    "    )\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial images and a csv file with the results\n",
    "We will now create the following numpy array:\n",
    "- images array  all generated adversarial images, for every possible $\\epsilon$ value\n",
    "- all_probs holding all found probalities for every class for every adversarial image\n",
    "- org_probs holding all originally found probabilties, only for the non adversarial images\n",
    "\n",
    "We will also create a csv using pandas with the following columns:\n",
    "- index: Index in the images and all_probs array\n",
    "- org_index: index in the org_probs array and the MNIST dataset of the original image\n",
    "- loss: loss for this step\n",
    "- eps: Used epsilon value for this adversarial image\n",
    "- correct: original label for this image\n",
    "- found_label: the label with the highest probablity for this image\n",
    "- Confidence: the probability of the found label\n",
    "- prob_0 t/m prob_10: probabilities found for all classes\n",
    "\n",
    "This csv can be used to obtain results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download mnist dataset\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "\n",
    "# Number of original images to walk over\n",
    "N_IMAGES = len(test)\n",
    "#keep data for csv dataframe\n",
    "data = []\n",
    "\n",
    "#Keep index of last created adversarial image\n",
    "image_idx  = 0\n",
    "\n",
    "#empty array that can hold all generated adversarial images\n",
    "images = np.zeros((len(GRADIENT_STEPS)*N_IMAGES,28**2)) \n",
    "#empty array that hold all probabiliteis for all genareted images\n",
    "all_probs = np.zeros((len(GRADIENT_STEPS)*N_IMAGES,10),dtype=np.float32) \n",
    "\n",
    "#empty array that keeps track of all original probabilities\n",
    "org_probs = np.zeros((N_IMAGES,10),dtype=np.float32)\n",
    "\n",
    "#Walk over the training image in batches of 128\n",
    "for i in range(0,int(math.ceil(N_IMAGES/128.))):\n",
    "    #start indexes of this batch\n",
    "    s = i * 128\n",
    "    # ends s+128 or N_images\n",
    "    e = min(s+128,N_IMAGES)\n",
    "    n = e-s\n",
    "    \n",
    "    #dataset consists of list of tuples, first element of tuple contains image \n",
    "    #second contains label\n",
    "    ins = test[s:e]\n",
    "    imgs = np.array([i[0] for i in ins])\n",
    "    labs = np.array([i[1] for i in ins],dtype=np.int32)\n",
    "    \n",
    "    # Calculate original probs and gradient with respect to input \n",
    "    loss, probs, gradients = pass_forward_sup(imgs, labs, True)\n",
    "    \n",
    "    # Adversarial images are generated using the signed gradient\n",
    "    sgrad = np.sign(gradients)\n",
    "    \n",
    "    # Save original probabilities, for later display\n",
    "    org_probs[s:e,:] = probs\n",
    "    \n",
    "    # we will generate the adversarial images in \n",
    "    for step in GRADIENT_STEPS:\n",
    "        # build images\n",
    "        advi = create_adv_images(imgs, sgrad, step)\n",
    "        # passes forward, giving probabilities and loss \n",
    "        loss, probs = pass_forward_sup(advi,labs)\n",
    "        # Loss is sum of loss of all different samples: should be corrected for batch size\n",
    "        loss = loss / n\n",
    "        # found labels with max probability\n",
    "        found_labels = np.argmax(probs,axis=1)\n",
    "        #walk over the 128 images seperately \n",
    "        for x in range(n):\n",
    "            #save iamges and probablities\n",
    "            images[image_idx,:] = advi[x]\n",
    "            all_probs[image_idx,:] = probs[x]\n",
    "            \n",
    "            #to csv\n",
    "            data.append([image_idx,s+x,loss, step, labs[x], found_labels[x],probs[x,found_labels[x]]] + probs[x,:].tolist())\n",
    "            image_idx +=1\n",
    "#save to csv       \n",
    "df = pd.DataFrame(data=data, columns=COLS)\n",
    "df.to_csv('supervised_images.csv', encoding='utf8')\n",
    "\n",
    "#save to npz files\n",
    "np.savez('supervised_advs.npz',images=images,org_probs=org_probs,all_probs=all_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing adversarial images\n",
    "I plot two images that are wrongly classified with $\\epsilon = 0.1$ , two that are wronly classified with $\\epsilon=0.25$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-436fa50a3951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mincorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfound_label\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#select that are classified correclty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# incorrectly classified anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def _plot_for(org_ids):\n",
    "    for org_id in org_ids:\n",
    "        selected_img = df[df.org_index == org_id].sort_values('eps')\n",
    "        imgs = np.take(images,selected_img.index.tolist(),axis=0)\n",
    "        conf = selected_img.confidence.tolist()\n",
    "        found_labels = selected_img.found_label.tolist()\n",
    "        plot_images(imgs, conf, found_labels)\n",
    "        \n",
    "incorrect = df[df.found_label != df.correct] #select that are classified correclty\n",
    "\n",
    "# incorrectly classified anyway\n",
    "incorrect_eps_0 = incorrect[incorrect.eps == 0.0] \n",
    "\n",
    "# Incorrectly classified with eps=0.1\n",
    "incorrect_eps_1 = incorrect[(incorrect.eps == 0.1) & (incorrect.org_index.isin(incorrect_eps_0.org_index) == False)]\n",
    "idx = incorrect_eps_1.sample(2).org_index.tolist()\n",
    "print \"Wrongly classified eps >= 0.1\", idx\n",
    "_plot_for(idx)\n",
    "\n",
    "# Incorrecly classfied with eps=0.25\n",
    "incorrect_eps_25 = incorrect[(incorrect.eps == 0.25)   & (incorrect.org_index.isin(incorrect_eps_0.org_index) == False) & (incorrect.org_index.isin(incorrect_eps_1.org_index) == False)]\n",
    "idx = incorrect_eps_25.sample(2).org_index.tolist()\n",
    "print \"Wrongly classified eps >= 0.25\", idx\n",
    "_plot_for(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "stats = statistics_from_df(df)\n",
    "statistics_to_latex_table(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "We will import the scripts directly for the purpose of the readability of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stijnvoss/Documents/UNI/CCN/git/end-assignment\n",
      "loading /Users/stijnvoss/Documents/UNI/CCN/git/end-assignment/improved-gan/train_mnist/mnist/discriminator.json\n",
      "loading /Users/stijnvoss/Documents/UNI/CCN/git/end-assignment/improved-gan/train_mnist/mnist/generator.json\n",
      "loading /Users/stijnvoss/Documents/UNI/CCN/git/end-assignment/improved-gan/train_mnist/mnist/generator.hdf5 ...\n",
      "loading /Users/stijnvoss/Documents/UNI/CCN/git/end-assignment/improved-gan/train_mnist/mnist/discriminator.hdf5 ...\n"
     ]
    }
   ],
   "source": [
    "repo_root = os.path.join(os.path.split(os.getcwd())[0],'improved-gan')\n",
    "mnist_root = os.path.join(repo_root,'train_mnist')\n",
    "# Overwrite system arguments such that argparse in train_mnist/args.py works correctly\n",
    "sys.argv = ['main','-g',str(GPU_DEVICE),'-m',os.path.join(mnist_root,'mnist'),'-p',os.path.join(mnist_root,'mnist_plot')]\n",
    "# add the imported repository to the path both the general code and the train_mnist specific code\n",
    "sys.path.append(repo_root)\n",
    "sys.path.append(os.path.join(repo_root,'train_mnist'))\n",
    "\n",
    "from gan import *\n",
    "from params import *\n",
    "# This will load the gan and the parameters learned during training\n",
    "from model import gan\n",
    "import mnist_tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adversarial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('supervised_advs.npz')\n",
    "images = data['images']\n",
    "supervised_df = pd.read_csv('supervised_images.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pass_forward_desc(ims, target_classes):\n",
    "    \"\"\" Will pass a batch of images trough the descriminator, will calculate loss and probabilities\n",
    "    \"\"\"\n",
    "    # converting to chainer variable makes sure we can obtain gradient\n",
    "    ims = gan.to_variable(ims) \n",
    "    \n",
    "    #Now pass forward trough the network and get probs, we do not use softmax since chainer only seems\n",
    "    #to provide a cross entropy softmax loss function\n",
    "    probs, activations = gan.discriminate(ims,test=True,apply_softmax=False) \n",
    "    # our cross entropy function that will calculate the loss also need chainer var\n",
    "    labels = gan.to_variable(target_classes)  \n",
    "\n",
    "    # Here we calculate the loss\n",
    "    loss = F.softmax_cross_entropy(probs, labels) \n",
    "\n",
    "    return gan.to_numpy(loss),gan.to_numpy(F.softmax(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [01:40<00:00, 451.34it/s]"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "gan_probs = np.zeros((len(images),10), dtype=np.float32)\n",
    "counter = 0\n",
    "b = tqdm(total=len(images))\n",
    "for t in supervised_df.itertuples():\n",
    "    #we have to multiple by 255. since our network is expecting in range 0-225.\n",
    "    img = 255. * np.take(images, t.index,axis=0).reshape(1,784).astype(np.float32)\n",
    "    \n",
    "    loss, probs = pass_forward_desc(img,np.array([t.correct],dtype=np.int32))\n",
    "    probs = probs[0]\n",
    "    gan_probs[t.index,:] = probs\n",
    "    found_label = np.argmax(probs)\n",
    "    data.append([t.index, t.org_index, float(loss), t.eps, t.correct, found_label, probs[found_label]] + list(probs))\n",
    "    b.update()\n",
    "df = pd.DataFrame(data=data, columns=COLS)\n",
    "df.to_csv('gan_results.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.6183, 0.99999177219271662, 6494.5943915863036), (0.1, 0.5544, 0.99999963449239726, 8511.3683567260741), (0.25, 0.4463, 0.99998241912722585, 12734.811778112793), (1.0, 0.1536, 0.9999874810039997, 41553.375895703124)]\n"
     ]
    }
   ],
   "source": [
    "print statistics_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
