{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing GAN results on MNIST\n",
    "The openAI foundation proposed an improved GAN and was able to apply it on the MNIST dataset. You can found the paper here: https://arxiv.org/abs/1606.03498. Someone else re-implemented the code in Chainer here: https://github.com/musyoku/improved-gan. However the code is quit hard to understand so i will first try to reproduce their results and understand what they did. The code is divided in general code that is used for all different GAN applications and models specific code. For example code that is used for the MNIST model in particular or generating anime faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some dependencies\n",
    "import math\n",
    "import numpy as np\n",
    "import chainer, os, collections, six, math, random, time, copy,sys\n",
    "from chainer import cuda, Variable, optimizers, serializers, function, optimizer, initializers\n",
    "from chainer.utils import type_check\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "# add the imported repository to the path, so we can always just import\n",
    "sys.path.append(os.path.join(os.path.split(os.getcwd())[0],'improved-gan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params\n",
    "They formalize the params of the discrimator, generator and classifier in classes. The formalized classes are then used as input by the general GAN code to fit the different applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base class\n",
    "# Found in params.py\n",
    "class Params():\n",
    "    def __init__(self, dict=None):\n",
    "        if dict:\n",
    "            self.from_dict(dict)\n",
    "\n",
    "    def from_dict(self, dict):\n",
    "        for attr, value in dict.iteritems():\n",
    "            if hasattr(self, attr):\n",
    "                setattr(self, attr, value)\n",
    "\n",
    "    def to_dict(self):\n",
    "        dict = {}\n",
    "        for attr, value in self.__dict__.iteritems():\n",
    "            if hasattr(value, \"to_dict\"):\n",
    "                dict[attr] = value.to_dict()\n",
    "            else:\n",
    "                dict[attr] = value\n",
    "        return dict\n",
    "\n",
    "    def dump(self):\n",
    "        for attr, value in self.__dict__.iteritems():\n",
    "            print \"\t{}: {}\".format(attr, value)\n",
    "\n",
    "# General GAN code (found in gan.py) :\n",
    "# These params can be defined for a Discriminator class\n",
    "class DiscriminatorParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 28 * 28\n",
    "        self.ndim_output = 10\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"elu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "        self.use_feature_matching = False\n",
    "        self.use_minibatch_discrimination = False\n",
    "\n",
    "# These params can be defined for a Generator class\n",
    "class GeneratorParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 10\n",
    "        self.ndim_output = 28 * 28\n",
    "        self.distribution_output = \"universal\"  # universal, sigmoid or tanh\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"relu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "\n",
    "# These parameters can \n",
    "class ClassifierParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 28 * 28\n",
    "        self.ndim_output = 10\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"elu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "        self.use_feature_matching = False\n",
    "        self.use_minibatch_discrimination = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentials\n",
    "The sequentials folder implements a lot of general neural network functionality to support the GAN model. For example a deconvolutional layer and weight normalization(https://arxiv.org/abs/1602.07868). I will not discuss all the code in detail since it's quit \n",
    "\n",
    "One important class is the Sequential class, which implements a sequence of neural network layer. It is loaded into a chain before optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General GAN\n",
    "The code below shows the general code that implements a GAN given the params defined above and a model for the discriminator and generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(sequential.Sequential):\n",
    "    \"\"\"\n",
    "    Sequential formalizes a sequence of neural network layers\n",
    "    \"\"\"\n",
    "    def __call__(self, x, test=False):\n",
    "        activations = []\n",
    "        for i, link in enumerate(self.links):\n",
    "            if isinstance(link, sequential.functions.dropout):\n",
    "                x = link(x, train=not test)\n",
    "            elif isinstance(link, chainer.links.BatchNormalization):\n",
    "                x = link(x, test=test)\n",
    "            else:\n",
    "                x = link(x)\n",
    "                if isinstance(link, sequential.functions.ActivationFunction):\n",
    "                    activations.append(x)\n",
    "        return x, activations\n",
    "\n",
    "# Following two help saving objects\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def to_object(dict):\n",
    "    obj = Object()\n",
    "    for key, value in dict.iteritems():\n",
    "        setattr(obj, key, value)\n",
    "    return obj\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, params_discriminator, params_generator):\n",
    "        \"\"\"\n",
    "        As an input a GAN gets two arguments: a dictionary for the discriminator and a dictionary for the generator\n",
    "        Both have two items with the key config and model. \n",
    "        The config key contains a param object implementing one of the param classes above\n",
    "        The model key contains a neural network, converted to a dictioniary via the Sequential implementation\n",
    "        \n",
    "        \"\"\"\n",
    "        self.params_discriminator = copy.deepcopy(params_discriminator)\n",
    "        self.config_discriminator = to_object(params_discriminator[\"config\"])\n",
    "\n",
    "        self.params_generator = copy.deepcopy(params_generator)\n",
    "        self.config_generator = to_object(params_generator[\"config\"])\n",
    "\n",
    "        self.build_discriminator()\n",
    "        self.build_generator()\n",
    "        self._gpu = False\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # discriminator model is extracted and loaded into a chain next we can build a optimizer\n",
    "        self.discriminator = sequential.chain.Chain()\n",
    "        self.discriminator.add_sequence(sequential.from_dict(self.params_discriminator[\"model\"]))\n",
    "        config = self.config_discriminator\n",
    "        self.discriminator.setup_optimizers(config.optimizer, config.learning_rate, config.momentum)\n",
    "\n",
    "    def build_generator(self):\n",
    "        #generator model is extracted and loaded into a chain next we can build a optimizer\n",
    "        self.generator = sequential.chain.Chain()\n",
    "        self.generator.add_sequence(sequential.from_dict(self.params_generator[\"model\"]))\n",
    "        config = self.config_discriminator\n",
    "        self.generator.setup_optimizers(config.optimizer, config.learning_rate, config.momentum)\n",
    "\n",
    "    def update_learning_rate(self, lr):\n",
    "        #Change learning rate of both discriminator and generator seperately\n",
    "        self.discriminator.update_learning_rate(lr)\n",
    "        self.generator.update_learning_rate(lr)\n",
    "\n",
    "    def to_gpu(self):\n",
    "        #Make sure both networks are trained on GPU\n",
    "        self.discriminator.to_gpu()\n",
    "        self.generator.to_gpu()\n",
    "        self._gpu = True\n",
    "\n",
    "    @property\n",
    "    def gpu_enabled(self):\n",
    "        # If gpu is set to true and cuda is available\n",
    "        if cuda.available is False:\n",
    "            return False\n",
    "        return self._gpu\n",
    "\n",
    "    @property\n",
    "    def xp(self):\n",
    "        # Get's cupy if gpu is enabled otherwise numpy\n",
    "        if self.gpu_enabled:\n",
    "            return cuda.cupy\n",
    "        return np\n",
    "\n",
    "    def to_variable(self, x):\n",
    "        # Helper function converts variable deals with gpu\n",
    "        if isinstance(x, Variable) == False:\n",
    "            x = Variable(x)\n",
    "            if self.gpu_enabled:\n",
    "                x.to_gpu()\n",
    "        return x\n",
    "\n",
    "    def to_numpy(self, x):\n",
    "        # helper functions converts to numpy deals with gpu\n",
    "        if isinstance(x, Variable) == True:\n",
    "            x = x.data\n",
    "        if isinstance(x, cuda.ndarray) == True:\n",
    "            x = cuda.to_cpu(x)\n",
    "        return x\n",
    "\n",
    "    def get_batchsize(self, x):\n",
    "        # Gets batch size\n",
    "        return x.shape[0]\n",
    "\n",
    "    def zero_grads(self):\n",
    "        # Reset all grads\n",
    "        self.optimizer_discriminator.zero_grads()\n",
    "        self.optimizer_generative_model.zero_grads()\n",
    "\n",
    "    def sample_z(self, batchsize=1):\n",
    "        \"\"\" Generates a random z sample from an uniform distribution\n",
    "        the gerenator will generate an image based on that input will use a complete batch\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config_generator\n",
    "        ndim_z = config.ndim_input\n",
    "        # uniform\n",
    "        z_batch = np.random.uniform(-1, 1, (batchsize, ndim_z)).astype(np.float32)\n",
    "        # gaussian\n",
    "        # z_batch = np.random.normal(0, 1, (batchsize, ndim_z)).astype(np.float32)\n",
    "        return z_batch\n",
    "\n",
    "    def generate_x(self, batchsize=1, test=False, as_numpy=False):\n",
    "        return self.generate_x_from_z(self.sample_z(batchsize), test=test, as_numpy=as_numpy)\n",
    "\n",
    "    def generate_x_from_z(self, z_batch, test=False, as_numpy=False):\n",
    "        z_batch = self.to_variable(z_batch)\n",
    "        x_batch, _ = self.generator(z_batch, test=test, return_activations=True)\n",
    "        if as_numpy:\n",
    "            return self.to_numpy(x_batch)\n",
    "        return x_batch\n",
    "\n",
    "    def discriminate(self, x_batch, test=False, apply_softmax=True):\n",
    "        x_batch = self.to_variable(x_batch)\n",
    "        prob, activations = self.discriminator(x_batch, test=test, return_activations=True)\n",
    "        if apply_softmax:\n",
    "            prob = F.softmax(prob)\n",
    "        return prob, activations\n",
    "\n",
    "    def backprop_discriminator(self, loss):\n",
    "        self.discriminator.backprop(loss)\n",
    "\n",
    "    def backprop_generator(self, loss):\n",
    "        self.generator.backprop(loss)\n",
    "\n",
    "    def compute_kld(self, p, q):\n",
    "        return F.reshape(F.sum(p * (F.log(p + 1e-16) - F.log(q + 1e-16)), axis=1), (-1, 1))\n",
    "\n",
    "    def get_unit_vector(self, v):\n",
    "        v /= (np.sqrt(np.sum(v ** 2, axis=1)).reshape((-1, 1)) + 1e-16)\n",
    "        return v\n",
    "\n",
    "    def compute_lds(self, x, xi=10, eps=1, Ip=1):\n",
    "        x = self.to_variable(x)\n",
    "        y1, _ = self.discriminate(x, apply_softmax=True)\n",
    "        y1.unchain_backward()\n",
    "        d = self.to_variable(self.get_unit_vector(np.random.normal(size=x.shape).astype(np.float32)))\n",
    "\n",
    "        for i in xrange(Ip):\n",
    "            y2, _ = self.discriminate(x + xi * d, apply_softmax=True)\n",
    "            kld = F.sum(self.compute_kld(y1, y2))\n",
    "            kld.backward()\n",
    "            d = self.to_variable(self.get_unit_vector(self.to_numpy(d.grad)))\n",
    "\n",
    "        y2, _ = self.discriminate(x + eps * d, apply_softmax=True)\n",
    "        return -self.compute_kld(y1, y2)\n",
    "\n",
    "    def load(self, dir=None):\n",
    "        if dir is None:\n",
    "            raise Exception()\n",
    "        self.generator.load(dir + \"/generator.hdf5\")\n",
    "        self.discriminator.load(dir + \"/discriminator.hdf5\")\n",
    "\n",
    "    def save(self, dir=None):\n",
    "        if dir is None:\n",
    "            raise Exception()\n",
    "        try:\n",
    "            os.mkdir(dir)\n",
    "        except:\n",
    "            pass\n",
    "        self.generator.save(dir + \"/generator.hdf5\")\n",
    "        self.discriminator.save(dir + \"/discriminator.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
