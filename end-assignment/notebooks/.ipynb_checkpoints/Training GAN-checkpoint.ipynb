{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GAN\n",
    "The openAI foundation proposed an improved GAN and was able to apply it on the MNIST dataset. You can found the paper here: https://arxiv.org/abs/1606.03498. Someone else re-implemented the code in Chainer here: https://github.com/musyoku/improved-gan. However the code is quit hard to understand so i will first try to reproduce their results and understand what they did. The code is divided in general code that is used for all different GAN applications and models specific code. For example code that is used for the MNIST model in particular or generating anime faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some dependencies\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import chainer, os, collections, six, math, random, time, copy,sys\n",
    "from chainer import cuda, Variable, optimizers, serializers, function, optimizer, initializers\n",
    "from chainer.utils import type_check\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "# add the imported repository to the path, so we can always just import\n",
    "sys.path.append(os.path.join(os.path.split(os.getcwd())[0],'improved-gan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params\n",
    "They formalize the params of the discrimator, generator and classifier in classes. The formalized classes are then used as input by the general GAN code to fit the different applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base class\n",
    "# Found in params.py\n",
    "class Params():\n",
    "    def __init__(self, dict=None):\n",
    "        if dict:\n",
    "            self.from_dict(dict)\n",
    "\n",
    "    def from_dict(self, dict):\n",
    "        for attr, value in dict.iteritems():\n",
    "            if hasattr(self, attr):\n",
    "                setattr(self, attr, value)\n",
    "\n",
    "    def to_dict(self):\n",
    "        dict = {}\n",
    "        for attr, value in self.__dict__.iteritems():\n",
    "            if hasattr(value, \"to_dict\"):\n",
    "                dict[attr] = value.to_dict()\n",
    "            else:\n",
    "                dict[attr] = value\n",
    "        return dict\n",
    "\n",
    "    def dump(self):\n",
    "        for attr, value in self.__dict__.iteritems():\n",
    "            print \"\t{}: {}\".format(attr, value)\n",
    "\n",
    "# General GAN code (found in gan.py) :\n",
    "# These params can be defined for a Discriminator class\n",
    "class DiscriminatorParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 28 * 28\n",
    "        self.ndim_output = 10\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"elu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "        self.use_feature_matching = False\n",
    "        self.use_minibatch_discrimination = False\n",
    "\n",
    "# These params can be defined for a Generator class\n",
    "class GeneratorParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 10\n",
    "        self.ndim_output = 28 * 28\n",
    "        self.distribution_output = \"universal\"  # universal, sigmoid or tanh\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"relu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "\n",
    "# These parameters can \n",
    "class ClassifierParams(Params):\n",
    "    def __init__(self):\n",
    "        self.ndim_input = 28 * 28\n",
    "        self.ndim_output = 10\n",
    "        self.weight_init_std = 1\n",
    "        self.weight_initializer = \"Normal\"  # Normal, GlorotNormal or HeNormal\n",
    "        self.nonlinearity = \"elu\"\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.gradient_clipping = 10\n",
    "        self.weight_decay = 0\n",
    "        self.use_feature_matching = False\n",
    "        self.use_minibatch_discrimination = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentials\n",
    "The sequentials folder implements a lot of general neural network functionality to support the GAN model. For example a deconvolutional layer and weight normalization(https://arxiv.org/abs/1602.07868). I will not discuss all the code in detail since it's quit \n",
    "\n",
    "One important class is the Sequential class, which implements a sequence of neural network layer. It is loaded into a chain before optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General GAN\n",
    "The code below shows the general code that implements a GAN given the params defined above and a model for the discriminator and generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(sequential.Sequential):\n",
    "    \"\"\"\n",
    "    Sequential formalizes a sequence of neural network layers\n",
    "    \"\"\"\n",
    "    def __call__(self, x, test=False):\n",
    "        activations = []\n",
    "        for i, link in enumerate(self.links):\n",
    "            if isinstance(link, sequential.functions.dropout):\n",
    "                x = link(x, train=not test)\n",
    "            elif isinstance(link, chainer.links.BatchNormalization):\n",
    "                x = link(x, test=test)\n",
    "            else:\n",
    "                x = link(x)\n",
    "                if isinstance(link, sequential.functions.ActivationFunction):\n",
    "                    activations.append(x)\n",
    "        return x, activations\n",
    "\n",
    "# Following two help saving objects\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def to_object(dict):\n",
    "    obj = Object()\n",
    "    for key, value in dict.iteritems():\n",
    "        setattr(obj, key, value)\n",
    "    return obj\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, params_discriminator, params_generator):\n",
    "        \"\"\"\n",
    "        As an input a GAN gets two arguments: a dictionary for the discriminator and a dictionary for the generator\n",
    "        Both have two items with the key config and model. \n",
    "        The config key contains a param object implementing one of the param classes above\n",
    "        The model key contains a neural network, converted to a dictioniary via the Sequential implementation\n",
    "        \n",
    "        \"\"\"\n",
    "        self.params_discriminator = copy.deepcopy(params_discriminator)\n",
    "        self.config_discriminator = to_object(params_discriminator[\"config\"])\n",
    "\n",
    "        self.params_generator = copy.deepcopy(params_generator)\n",
    "        self.config_generator = to_object(params_generator[\"config\"])\n",
    "\n",
    "        self.build_discriminator()\n",
    "        self.build_generator()\n",
    "        self._gpu = False\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # discriminator model is extracted and loaded into a chain next we can build a optimizer\n",
    "        self.discriminator = sequential.chain.Chain()\n",
    "        self.discriminator.add_sequence(sequential.from_dict(self.params_discriminator[\"model\"]))\n",
    "        config = self.config_discriminator\n",
    "        self.discriminator.setup_optimizers(config.optimizer, config.learning_rate, config.momentum)\n",
    "\n",
    "    def build_generator(self):\n",
    "        #generator model is extracted and loaded into a chain next we can build a optimizer\n",
    "        self.generator = sequential.chain.Chain()\n",
    "        self.generator.add_sequence(sequential.from_dict(self.params_generator[\"model\"]))\n",
    "        config = self.config_discriminator\n",
    "        self.generator.setup_optimizers(config.optimizer, config.learning_rate, config.momentum)\n",
    "\n",
    "    def update_learning_rate(self, lr):\n",
    "        #Change learning rate of both discriminator and generator seperately\n",
    "        self.discriminator.update_learning_rate(lr)\n",
    "        self.generator.update_learning_rate(lr)\n",
    "\n",
    "    def to_gpu(self):\n",
    "        #Make sure both networks are trained on GPU\n",
    "        self.discriminator.to_gpu()\n",
    "        self.generator.to_gpu()\n",
    "        self._gpu = True\n",
    "\n",
    "    @property\n",
    "    def gpu_enabled(self):\n",
    "        # If gpu is set to true and cuda is available\n",
    "        if cuda.available is False:\n",
    "            return False\n",
    "        return self._gpu\n",
    "\n",
    "    @property\n",
    "    def xp(self):\n",
    "        # Get's cupy if gpu is enabled otherwise numpy\n",
    "        if self.gpu_enabled:\n",
    "            return cuda.cupy\n",
    "        return np\n",
    "\n",
    "    def to_variable(self, x):\n",
    "        # Helper function converts variable deals with gpu\n",
    "        if isinstance(x, Variable) == False:\n",
    "            x = Variable(x)\n",
    "            if self.gpu_enabled:\n",
    "                x.to_gpu()\n",
    "        return x\n",
    "\n",
    "    def to_numpy(self, x):\n",
    "        # helper functions converts to numpy deals with gpu\n",
    "        if isinstance(x, Variable) == True:\n",
    "            x = x.data\n",
    "        if isinstance(x, cuda.ndarray) == True:\n",
    "            x = cuda.to_cpu(x)\n",
    "        return x\n",
    "\n",
    "    def get_batchsize(self, x):\n",
    "        # Gets batch size\n",
    "        return x.shape[0]\n",
    "\n",
    "    def zero_grads(self):\n",
    "        # Reset all grads\n",
    "        self.optimizer_discriminator.zero_grads()\n",
    "        self.optimizer_generative_model.zero_grads()\n",
    "\n",
    "    def sample_z(self, batchsize=1):\n",
    "        \"\"\" Generates a random z sample from an uniform distribution\n",
    "        the gerenator will generate an image based on that input will use a complete batch\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config_generator\n",
    "        ndim_z = config.ndim_input\n",
    "        # uniform\n",
    "        z_batch = np.random.uniform(-1, 1, (batchsize, ndim_z)).astype(np.float32)\n",
    "        # gaussian\n",
    "        # z_batch = np.random.normal(0, 1, (batchsize, ndim_z)).astype(np.float32)\n",
    "        return z_batch\n",
    "\n",
    "    def generate_x(self, batchsize=1, test=False, as_numpy=False):\n",
    "        \"\"\"\n",
    "        This function lets the generator generate a variable\n",
    "        It combines the input from sample_z with generate_x_from_z to generator z\n",
    "        \"\"\"\n",
    "        return self.generate_x_from_z(self.sample_z(batchsize), test=test, as_numpy=as_numpy)\n",
    "\n",
    "    def generate_x_from_z(self, z_batch, test=False, as_numpy=False):\n",
    "        \"\"\"\n",
    "        This functions generates x a sample by the generator given a random input z. \n",
    "        it will automatically get the batch size from z_batch\n",
    "        \"\"\"\n",
    "        z_batch = self.to_variable(z_batch)\n",
    "        x_batch, _ = self.generator(z_batch, test=test, return_activations=True)\n",
    "        if as_numpy:\n",
    "            return self.to_numpy(x_batch)\n",
    "        return x_batch\n",
    "\n",
    "    def discriminate(self, x_batch, test=False, apply_softmax=True):\n",
    "        \"\"\"\n",
    "        Given an example produced by the generator (likely by using generate_x), will call \n",
    "        the discriminator and return an probability to see if this is a fake our another class\n",
    "        \"\"\"\n",
    "        x_batch = self.to_variable(x_batch)\n",
    "        prob, activations = self.discriminator(x_batch, test=test, return_activations=True)\n",
    "        if apply_softmax:\n",
    "            prob = F.softmax(prob)\n",
    "        return prob, activations\n",
    "\n",
    "    def backprop_discriminator(self, loss):\n",
    "        # Backpropagate and learn through discriminator\n",
    "        self.discriminator.backprop(loss)\n",
    "\n",
    "    def backprop_generator(self, loss):\n",
    "        # Backpropagate and learn generator\n",
    "        self.generator.backprop(loss)\n",
    "\n",
    "    def compute_kld(self, p, q):\n",
    "        # Helper function that calculates \n",
    "        return F.reshape(F.sum(p * (F.log(p + 1e-16) - F.log(q + 1e-16)), axis=1), (-1, 1))\n",
    "\n",
    "    def get_unit_vector(self, v):\n",
    "        v /= (np.sqrt(np.sum(v ** 2, axis=1)).reshape((-1, 1)) + 1e-16)\n",
    "        return v\n",
    "\n",
    "    def compute_lds(self, x, xi=10, eps=1, Ip=1):\n",
    "        x = self.to_variable(x)\n",
    "        y1, _ = self.discriminate(x, apply_softmax=True)\n",
    "        y1.unchain_backward()\n",
    "        d = self.to_variable(self.get_unit_vector(np.random.normal(size=x.shape).astype(np.float32)))\n",
    "\n",
    "        for i in xrange(Ip):\n",
    "            y2, _ = self.discriminate(x + xi * d, apply_softmax=True)\n",
    "            kld = F.sum(self.compute_kld(y1, y2))\n",
    "            kld.backward()\n",
    "            d = self.to_variable(self.get_unit_vector(self.to_numpy(d.grad)))\n",
    "\n",
    "        y2, _ = self.discriminate(x + eps * d, apply_softmax=True)\n",
    "        return -self.compute_kld(y1, y2)\n",
    "\n",
    "    def load(self, dir=None):\n",
    "        if dir is None:\n",
    "            raise Exception()\n",
    "        self.generator.load(dir + \"/generator.hdf5\")\n",
    "        self.discriminator.load(dir + \"/discriminator.hdf5\")\n",
    "\n",
    "    def save(self, dir=None):\n",
    "        if dir is None:\n",
    "            raise Exception()\n",
    "        try:\n",
    "            os.mkdir(dir)\n",
    "        except:\n",
    "            pass\n",
    "        self.generator.save(dir + \"/generator.hdf5\")\n",
    "        self.discriminator.save(dir + \"/discriminator.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST\n",
    "Next we will look how they use these general classes to implement MNIST training\n",
    "### Args\n",
    "The provided code assumes that the terminal is used and uses args.py to parse the arguments. Since i work in a notebook i solve it by manually crafting an args object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.split(os.getcwd())[0],'improved-gan/train_mnist'))\n",
    "args = Object()\n",
    "args.model_dir = 'mnist'\n",
    "args.gpu_device = 0\n",
    "args.seed = None\n",
    "args.plot_dir = 'mnist-plot'\n",
    "args.num_labeled = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "This defines the generator and descriminator file. It saves the params and model to json and afterwards loads from this again. This makes it easy to change the model to your needs.\n",
    "\n",
    "The discriminator gets a 28x28 image input flattened to a 756x1 vector and outputs one vector of 10x1: a value for each of the classes. It consists of three fully connected hidden layers 756x1000x500x250x10. \n",
    "\n",
    "The generator gets a 50x1 vector latent dimensional input(random variable) and produces a 756 output(the 28x28 image). It also has two hidden layers: 50x500x500x756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist/discriminator.json\n",
      "loading mnist/generator.json\n",
      "loading mnist/generator.hdf5 ...\n",
      "loading mnist/discriminator.hdf5 ...\n"
     ]
    }
   ],
   "source": [
    "from sequential import Sequential\n",
    "from sequential.layers import Linear, BatchNormalization, MinibatchDiscrimination\n",
    "from sequential.functions import Activation, dropout, gaussian_noise, softmax\n",
    "\n",
    "# load params.json\n",
    "try:\n",
    "    os.mkdir(args.model_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# data\n",
    "image_width = 28\n",
    "image_height = image_width\n",
    "ndim_latent_code = 50 # 50 latent dimensional input\n",
    "\n",
    "# specify discriminator\n",
    "discriminator_sequence_filename = args.model_dir + \"/discriminator.json\"\n",
    "\n",
    "if os.path.isfile(discriminator_sequence_filename):\n",
    "    print \"loading\", discriminator_sequence_filename\n",
    "    with open(discriminator_sequence_filename, \"r\") as f:\n",
    "        try:\n",
    "            params = json.load(f)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"could not load {}\".format(discriminator_sequence_filename))\n",
    "else:\n",
    "    config = ClassifierParams()\n",
    "    config.ndim_input = image_width * image_height\n",
    "    config.ndim_output = 10\n",
    "    config.weight_init_std = 1\n",
    "    config.weight_initializer = \"GlorotNormal\"\n",
    "    config.use_weightnorm = False\n",
    "    config.nonlinearity = \"softplus\"\n",
    "    config.optimizer = \"Adam\"\n",
    "    config.learning_rate = 0.001\n",
    "    config.momentum = 0.5\n",
    "    config.gradient_clipping = 10\n",
    "    config.weight_decay = 0\n",
    "    config.use_feature_matching = True\n",
    "    config.use_minibatch_discrimination = False\n",
    "\n",
    "    discriminator = Sequential(weight_initializer=config.weight_initializer, weight_init_std=config.weight_init_std)\n",
    "    discriminator.add(gaussian_noise(std=0.3))\n",
    "    discriminator.add(Linear(config.ndim_input, 1000, use_weightnorm=config.use_weightnorm))\n",
    "    discriminator.add(gaussian_noise(std=0.5))\n",
    "    discriminator.add(Activation(config.nonlinearity))\n",
    "    # discriminator.add(BatchNormalization(1000))\n",
    "    discriminator.add(Linear(None, 500, use_weightnorm=config.use_weightnorm))\n",
    "    discriminator.add(gaussian_noise(std=0.5))\n",
    "    discriminator.add(Activation(config.nonlinearity))\n",
    "    # discriminator.add(BatchNormalization(500))\n",
    "    discriminator.add(Linear(None, 250, use_weightnorm=config.use_weightnorm))\n",
    "    discriminator.add(gaussian_noise(std=0.5))\n",
    "    discriminator.add(Activation(config.nonlinearity))\n",
    "    # discriminator.add(BatchNormalization(250))\n",
    "    if config.use_minibatch_discrimination:\n",
    "        discriminator.add(MinibatchDiscrimination(None, num_kernels=50, ndim_kernel=5))\n",
    "    discriminator.add(Linear(None, config.ndim_output, use_weightnorm=config.use_weightnorm))\n",
    "    # no need to add softmax() here\n",
    "    discriminator.build()\n",
    "\n",
    "    params = {\n",
    "        \"config\": config.to_dict(),\n",
    "        \"model\": discriminator.to_dict(),\n",
    "    }\n",
    "\n",
    "    with open(discriminator_sequence_filename, \"w\") as f:\n",
    "        json.dump(params, f, indent=4, sort_keys=True, separators=(',', ': '))\n",
    "\n",
    "discriminator_params = params\n",
    "\n",
    "# specify generator\n",
    "generator_sequence_filename = args.model_dir + \"/generator.json\"\n",
    "\n",
    "if os.path.isfile(generator_sequence_filename):\n",
    "    print \"loading\", generator_sequence_filename\n",
    "    with open(generator_sequence_filename, \"r\") as f:\n",
    "        try:\n",
    "            params = json.load(f)\n",
    "        except:\n",
    "            raise Exception(\"could not load {}\".format(generator_sequence_filename))\n",
    "else:\n",
    "    config = GeneratorParams()\n",
    "    config.ndim_input = ndim_latent_code\n",
    "    config.ndim_output = image_width * image_height\n",
    "    config.distribution_output = \"tanh\"\n",
    "    config.use_weightnorm = False\n",
    "    config.weight_init_std = 1\n",
    "    config.weight_initializer = \"GlorotNormal\"\n",
    "    config.nonlinearity = \"relu\"\n",
    "    config.optimizer = \"Adam\"\n",
    "    config.learning_rate = 0.001\n",
    "    config.momentum = 0.5\n",
    "    config.gradient_clipping = 10\n",
    "    config.weight_decay = 0\n",
    "\n",
    "    # generator\n",
    "    generator = Sequential(weight_initializer=config.weight_initializer, weight_init_std=config.weight_init_std)\n",
    "    generator.add(Linear(config.ndim_input, 500, use_weightnorm=config.use_weightnorm))\n",
    "    generator.add(BatchNormalization(500))\n",
    "    generator.add(Activation(config.nonlinearity))\n",
    "    generator.add(Linear(None, 500, use_weightnorm=config.use_weightnorm))\n",
    "    generator.add(BatchNormalization(500))\n",
    "    generator.add(Activation(config.nonlinearity))\n",
    "    generator.add(Linear(None, config.ndim_output, use_weightnorm=config.use_weightnorm))\n",
    "    if config.distribution_output == \"sigmoid\":\n",
    "        generator.add(Activation(\"sigmoid\"))\n",
    "    if config.distribution_output == \"tanh\":\n",
    "        generator.add(Activation(\"tanh\"))\n",
    "    generator.build()\n",
    "\n",
    "    params = {\n",
    "        \"config\": config.to_dict(),\n",
    "        \"model\": generator.to_dict(),\n",
    "    }\n",
    "\n",
    "    with open(generator_sequence_filename, \"w\") as f:\n",
    "        json.dump(params, f, indent=4, sort_keys=True, separators=(',', ': '))\n",
    "\n",
    "generator_params = params\n",
    "\n",
    "gan = GAN(discriminator_params, generator_params)\n",
    "gan.load(args.model_dir)\n",
    "\n",
    "if args.gpu_device != -1:\n",
    "    cuda.get_device(args.gpu_device).use()\n",
    "    gan.to_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset\n",
    "Creates a semi-supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_tools\n",
    "def load_train_images():\n",
    "    return mnist_tools.load_train_images()\n",
    "\n",
    "\n",
    "def load_test_images():\n",
    "    return mnist_tools.load_test_images()\n",
    "\n",
    "def binarize_data(x):\n",
    "    threshold = np.random.uniform(size=x.shape)\n",
    "    return np.where(threshold < x, 1.0, 0.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def create_semisupervised(images, labels, num_validation_data=10000, num_labeled_data=100, num_types_of_label=10,\n",
    "                          seed=0):\n",
    "    if len(images) < num_validation_data + num_labeled_data:\n",
    "        raise Exception(\"len(images) < num_validation_data + num_labeled_data\")\n",
    "    training_labeled_x = []\n",
    "    training_unlabeled_x = []\n",
    "    validation_x = []\n",
    "    validation_labels = []\n",
    "    training_labels = []\n",
    "    indices_for_label = {}\n",
    "    num_data_per_label = int(num_labeled_data / num_types_of_label)\n",
    "    num_unlabeled_data = len(images) - num_validation_data - num_labeled_data\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(images))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    def check(index):\n",
    "        label = labels[index]\n",
    "        if label not in indices_for_label:\n",
    "            indices_for_label[label] = []\n",
    "            return True\n",
    "        if len(indices_for_label[label]) < num_data_per_label:\n",
    "            for i in indices_for_label[label]:\n",
    "                if i == index:\n",
    "                    return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    for n in xrange(len(images)):\n",
    "        index = indices[n]\n",
    "        if check(index):\n",
    "            indices_for_label[labels[index]].append(index)\n",
    "            training_labeled_x.append(images[index])\n",
    "            training_labels.append(labels[index])\n",
    "        else:\n",
    "            if len(training_unlabeled_x) < num_unlabeled_data:\n",
    "                training_unlabeled_x.append(images[index])\n",
    "            else:\n",
    "                validation_x.append(images[index])\n",
    "                validation_labels.append(labels[index])\n",
    "\n",
    "    # reset seed\n",
    "    np.random.seed()\n",
    "\n",
    "    return training_labeled_x, training_labels, training_unlabeled_x, validation_x, validation_labels\n",
    "\n",
    "\n",
    "def sample_labeled_data(images, labels, batchsize, ndim_x, ndim_y, binarize=True):\n",
    "    image_batch = np.zeros((batchsize, ndim_x), dtype=np.float32)\n",
    "    label_onehot_batch = np.zeros((batchsize, ndim_y), dtype=np.float32)\n",
    "    label_id_batch = np.zeros((batchsize,), dtype=np.int32)\n",
    "    indices = np.random.choice(np.arange(len(images), dtype=np.int32), size=batchsize, replace=False)\n",
    "    for j in range(batchsize):\n",
    "        data_index = indices[j]\n",
    "        img = images[data_index].astype(np.float32) / 255.0\n",
    "        image_batch[j] = img.reshape((ndim_x,))\n",
    "        label_onehot_batch[j, labels[data_index]] = 1\n",
    "        label_id_batch[j] = labels[data_index]\n",
    "    if binarize:\n",
    "        image_batch = binarize_data(image_batch)\n",
    "    # [0, 1] -> [-1, 1]\n",
    "    image_batch = image_batch * 2.0 - 1.0\n",
    "    return image_batch, label_onehot_batch, label_id_batch\n",
    "\n",
    "\n",
    "def sample_unlabeled_data(images, batchsize, ndim_x, binarize=True):\n",
    "    image_batch = np.zeros((batchsize, ndim_x), dtype=np.float32)\n",
    "    indices = np.random.choice(np.arange(len(images), dtype=np.int32), size=batchsize, replace=False)\n",
    "    for j in range(batchsize):\n",
    "        data_index = indices[j]\n",
    "        img = images[data_index].astype(np.float32) / 255.0\n",
    "        image_batch[j] = img.reshape((ndim_x,))\n",
    "    if binarize:\n",
    "        image_batch = binarize_data(image_batch)\n",
    "    # [0, 1] -> [-1, 1]\n",
    "    image_batch = image_batch * 2.0 - 1.0\n",
    "    return image_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import visualizer\n",
    "from progress import Progress\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "\n",
    "\n",
    "def plot(filename=\"gen\"):\n",
    "    try:\n",
    "        os.mkdir(args.plot_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x_fake = gan.generate_x(100, test=True, as_numpy=True)\n",
    "    x_fake = (x_fake + 1.0) / 2.0\n",
    "    visualizer.tile_binary_images(x_fake.reshape((-1, 28, 28)), dir=args.plot_dir, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_learning_rate_for_epoch(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.001\n",
    "    if epoch < 50:\n",
    "        return 0.0003\n",
    "    return 0.0001\n",
    "\n",
    "\n",
    "def main():\n",
    "    # load MNIST images\n",
    "    images, labels = load_train_images()\n",
    "\n",
    "    # config\n",
    "    discriminator_config = gan.config_discriminator\n",
    "    generator_config = gan.config_generator\n",
    "\n",
    "    # settings\n",
    "    # _l -> labeled\n",
    "    # _u -> unlabeled\n",
    "    # _g -> generated\n",
    "    max_epoch = 1000\n",
    "    num_trains_per_epoch = 500\n",
    "    plot_interval = 5\n",
    "    batchsize_l = 100\n",
    "    batchsize_u = 100\n",
    "    batchsize_g = batchsize_u\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    if args.gpu_device != -1:\n",
    "        cuda.cupy.random.seed(args.seed)\n",
    "\n",
    "    # save validation accuracy per epoch\n",
    "    csv_results = []\n",
    "\n",
    "    # create semi-supervised split\n",
    "    num_validation_data = 10000\n",
    "    num_labeled_data = args.num_labeled\n",
    "    if batchsize_l > num_labeled_data:\n",
    "        batchsize_l = num_labeled_data\n",
    "\n",
    "    training_images_l, training_labels_l, training_images_u, validation_images, validation_labels = create_semisupervised(\n",
    "        images, labels, num_validation_data, num_labeled_data, discriminator_config.ndim_output, seed=args.seed)\n",
    "    print training_labels_l\n",
    "\n",
    "    # training\n",
    "    progress = Progress()\n",
    "    for epoch in xrange(1, max_epoch):\n",
    "        progress.start_epoch(epoch, max_epoch)\n",
    "        sum_loss_supervised = 0\n",
    "        sum_loss_unsupervised = 0\n",
    "        sum_loss_adversarial = 0\n",
    "        sum_dx_labeled = 0\n",
    "        sum_dx_unlabeled = 0\n",
    "        sum_dx_generated = 0\n",
    "\n",
    "        gan.update_learning_rate(get_learning_rate_for_epoch(epoch))\n",
    "\n",
    "        for t in xrange(num_trains_per_epoch):\n",
    "            # sample from data distribution\n",
    "            images_l, label_onehot_l, label_ids_l = sample_labeled_data(training_images_l, training_labels_l,\n",
    "                                                                                batchsize_l,\n",
    "                                                                                discriminator_config.ndim_input,\n",
    "                                                                                discriminator_config.ndim_output,\n",
    "                                                                                binarize=False)\n",
    "            images_u = sample_unlabeled_data(training_images_u, batchsize_u, discriminator_config.ndim_input,\n",
    "                                                     binarize=False)\n",
    "            images_g = gan.generate_x(batchsize_g)\n",
    "            images_g.unchain_backward()\n",
    "\n",
    "            # supervised loss\n",
    "            py_x_l, activations_l = gan.discriminate(images_l, apply_softmax=False)\n",
    "            loss_supervised = F.softmax_cross_entropy(py_x_l, gan.to_variable(label_ids_l))\n",
    "\n",
    "            log_zx_l = F.logsumexp(py_x_l, axis=1)\n",
    "            log_dx_l = log_zx_l - F.softplus(log_zx_l)\n",
    "            dx_l = F.sum(F.exp(log_dx_l)) / batchsize_l\n",
    "\n",
    "            # unsupervised loss\n",
    "            # D(x) = Z(x) / {Z(x) + 1}, where Z(x) = \\sum_{k=1}^K exp(l_k(x))\n",
    "            # softplus(x) := log(1 + exp(x))\n",
    "            # logD(x) = logZ(x) - log(Z(x) + 1)\n",
    "            # \t\t  = logZ(x) - log(exp(log(Z(x))) + 1)\n",
    "            # \t\t  = logZ(x) - softplus(logZ(x))\n",
    "            # 1 - D(x) = 1 / {Z(x) + 1}\n",
    "            # log{1 - D(x)} = log1 - log(Z(x) + 1)\n",
    "            # \t\t\t\t= -log(exp(log(Z(x))) + 1)\n",
    "            # \t\t\t\t= -softplus(logZ(x))\n",
    "            py_x_u, _ = gan.discriminate(images_u, apply_softmax=False)\n",
    "            log_zx_u = F.logsumexp(py_x_u, axis=1)\n",
    "            log_dx_u = log_zx_u - F.softplus(log_zx_u)\n",
    "            dx_u = F.sum(F.exp(log_dx_u)) / batchsize_u\n",
    "            loss_unsupervised = -F.sum(log_dx_u) / batchsize_u  # minimize negative logD(x)\n",
    "            py_x_g, _ = gan.discriminate(images_g, apply_softmax=False)\n",
    "            log_zx_g = F.logsumexp(py_x_g, axis=1)\n",
    "            loss_unsupervised += F.sum(F.softplus(log_zx_g)) / batchsize_u  # minimize negative log{1 - D(x)}\n",
    "\n",
    "            # update discriminator\n",
    "            gan.backprop_discriminator(loss_supervised + loss_unsupervised)\n",
    "\n",
    "            # adversarial loss\n",
    "            images_g = gan.generate_x(batchsize_g)\n",
    "            py_x_g, activations_g = gan.discriminate(images_g, apply_softmax=False)\n",
    "            log_zx_g = F.logsumexp(py_x_g, axis=1)\n",
    "            log_dx_g = log_zx_g - F.softplus(log_zx_g)\n",
    "            dx_g = F.sum(F.exp(log_dx_g)) / batchsize_g\n",
    "            loss_adversarial = -F.sum(log_dx_g) / batchsize_u  # minimize negative logD(x)\n",
    "\n",
    "            # feature matching\n",
    "            if discriminator_config.use_feature_matching:\n",
    "                features_true = activations_l[-1]\n",
    "                features_true.unchain_backward()\n",
    "                if batchsize_l != batchsize_g:\n",
    "                    images_g = gan.generate_x(batchsize_l)\n",
    "                    _, activations_g = gan.discriminate(images_g, apply_softmax=False)\n",
    "                features_fake = activations_g[-1]\n",
    "                loss_adversarial += F.mean_squared_error(features_true, features_fake)\n",
    "\n",
    "            # update generator\n",
    "            gan.backprop_generator(loss_adversarial)\n",
    "\n",
    "            sum_loss_supervised += float(loss_supervised.data)\n",
    "            sum_loss_unsupervised += float(loss_unsupervised.data)\n",
    "            sum_loss_adversarial += float(loss_adversarial.data)\n",
    "            sum_dx_labeled += float(dx_l.data)\n",
    "            sum_dx_unlabeled += float(dx_u.data)\n",
    "            sum_dx_generated += float(dx_g.data)\n",
    "            if t % 10 == 0:\n",
    "                progress.show(t, num_trains_per_epoch, {})\n",
    "\n",
    "        gan.save(args.model_dir)\n",
    "\n",
    "        # validation\n",
    "        images_l, _, label_ids_l = sample_labeled_data(validation_images, validation_labels,\n",
    "                                                               num_validation_data, discriminator_config.ndim_input,\n",
    "                                                               discriminator_config.ndim_output, binarize=False)\n",
    "        images_l_segments = np.split(images_l, num_validation_data // 500)\n",
    "        label_ids_l_segments = np.split(label_ids_l, num_validation_data // 500)\n",
    "        sum_accuracy = 0\n",
    "        for images_l, label_ids_l in zip(images_l_segments, label_ids_l_segments):\n",
    "            y_distribution, _ = gan.discriminate(images_l, apply_softmax=True, test=True)\n",
    "            accuracy = F.accuracy(y_distribution, gan.to_variable(label_ids_l))\n",
    "            sum_accuracy += float(accuracy.data)\n",
    "        validation_accuracy = sum_accuracy / len(images_l_segments)\n",
    "\n",
    "        progress.show(num_trains_per_epoch, num_trains_per_epoch, {\n",
    "            \"loss_l\": sum_loss_supervised / num_trains_per_epoch,\n",
    "            \"loss_u\": sum_loss_unsupervised / num_trains_per_epoch,\n",
    "            \"loss_g\": sum_loss_adversarial / num_trains_per_epoch,\n",
    "            \"dx_l\": sum_dx_labeled / num_trains_per_epoch,\n",
    "            \"dx_u\": sum_dx_unlabeled / num_trains_per_epoch,\n",
    "            \"dx_g\": sum_dx_generated / num_trains_per_epoch,\n",
    "            \"accuracy\": validation_accuracy,\n",
    "        })\n",
    "\n",
    "        # write accuracy to csv\n",
    "        csv_results.append([epoch, validation_accuracy, progress.get_total_time()])\n",
    "        data = pd.DataFrame(csv_results)\n",
    "        data.columns = [\"epoch\", \"accuracy\", \"min\"]\n",
    "        data.to_csv(\"{}/result.csv\".format(args.model_dir))\n",
    "\n",
    "        if epoch % plot_interval == 0 or epoch == 1:\n",
    "            plot(filename=\"epoch_{}_time_{}min\".format(epoch, progress.get_total_time()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
